# Progress Log

## Learnings
- Bash `((VAR++))` returns exit 1 when VAR is 0 under `set -e`. Use `VAR=$((VAR + 1))` instead.
- JSON schemas: keep them self-contained (inline enums rather than cross-file refs) for standalone usability.
- When mixing file types in a directory (feedback records + registry), filter by schema fields (e.g., `select(.bead != null)`) to avoid processing non-target files.

---

## Iteration 1 - US-101: Define outcome and feedback JSON schemas
- Created `config/schemas/outcome.json` with 5 outcome types (full_pass, partial_pass, agent_failure, infra_failure, timeout) and schema_version
- Created `config/schemas/feedback.json` with all required fields (bead, timestamp, template, agent, model, outcome, signals, failure_patterns, prompt_hash) and 7 signal sub-fields
- Created `tests/test-schemas.sh` with 27 assertions covering structure, fields, sub-fields, and cross-references
- Fixed bash arithmetic pitfall: `((PASS++))` → `PASS=$((PASS + 1))` for `set -e` compatibility
**Summary:** Task: US-101 | Files: [config/schemas/outcome.json, config/schemas/feedback.json, tests/test-schemas.sh] | Tests: PASS (27/27) | Review: PASSED | Next: US-102
---

## Iteration 2 - US-102: Build feedback-collector.sh
- Created `scripts/feedback-collector.sh` (~110 lines): reads run record JSON, extracts signals, classifies outcome, outputs feedback JSON
- Outcome classification priority chain: timeout → infra_failure (by failure_reason) → agent_failure (exit_code!=0 or status=failed) → full_pass (all checks pass) → partial_pass
- Signals extracted: exit_clean, tests_pass, lint_pass, ubs_clean, truthsayer_clean, duration_ratio (vs 600s baseline), retried (attempt>1)
- Skips runs with status=running (no output produced)
- FEEDBACK_DIR env var overrides output path for testability
- Created `tests/test-feedback-collector.sh` with 38 assertions covering all 5 outcome types, signal extraction, retry detection, running skip, JSON validity, and usage message
- Smoke-tested against real run record bd-10p: correctly classified as partial_pass
- Learnings: Real run records have verification embedded (not separate file). failure_reason values like "tmux-launch-failed", "status-file" map to infra_failure.
**Summary:** Task: US-102 | Files: [scripts/feedback-collector.sh, tests/test-feedback-collector.sh] | Tests: PASS (38/38) | Review: PASSED | Next: US-103
---

## Iteration 3 - US-103: Build failure pattern detector
- Created `scripts/detect-patterns.sh` (~147 lines): standalone script that detects 8 failure patterns from run record JSON
- Patterns: test-failure-after-completion, lint-failure-after-completion, scope-creep (>1800s), incomplete-work (2+ verif fails), infra-tmux, infra-disk, repeated-failure (attempt>1), verification-gap (2+ skipped)
- Output: JSON array of detected pattern names to stdout
- `--update-registry` flag writes/updates `pattern-registry.json` with count, first_seen, last_seen, last_beads (max 10)
- REGISTRY_FILE env var overrides registry path for testability
- Integrated into `feedback-collector.sh`: failure_patterns field now populated via detect-patterns.sh call
- Created `tests/test-detect-patterns.sh` with 25 assertions covering all 8 patterns, clean runs, multiple patterns, JSON format, registry accumulation, and usage
- All 90 tests pass across 3 suites (25 + 38 + 27)
- Learnings: When test fixtures omit optional fields like truthsayer, jq `// "skipped"` default triggers verification-gap detection. Always include all fields in fixtures for accurate testing.
**Summary:** Task: US-103 | Files: [scripts/detect-patterns.sh, tests/test-detect-patterns.sh, scripts/feedback-collector.sh] | Tests: PASS (25/25) | Review: PASSED | Next: US-104
---

## Iteration 4 - US-104: Build score-templates.sh
- Created `scripts/score-templates.sh` (~115 lines): aggregates feedback records into template scores using a single jq pipeline
- Composite score formula: `(full_pass_rate × 1.0) + (partial_pass_rate × 0.4) - (retry_rate × 0.2) - (timeout_rate × 0.3)` clamped [0, 1]
- Infra failures excluded from scoring (scoreable_runs tracks non-infra count)
- Confidence levels: low (<5), medium (5-19), high (≥20)
- Trend detection: compares last-10 full_pass_rate vs all-time, ±0.05 threshold → improving/declining/stable/insufficient_data
- Per-agent breakdown reuses compute_score() for zero duplication
- FEEDBACK_DIR and SCORES_DIR env vars for testability, consistent with other scripts
- Created `tests/test-score-templates.sh` with 30 assertions covering: empty input, all-pass, mixed outcomes, confidence levels, retry impact, score clamping, multiple templates, agent breakdown, trend detection (improving/declining/stable/insufficient_data), infra exclusion
- All 120 tests pass across 4 suites (27 + 38 + 25 + 30)
- Learnings: Use `shopt -s nullglob` instead of `2>/dev/null` on bash array glob assignments — the latter causes syntax errors. Sorting by bead name is a reasonable chronological proxy when timestamps aren't granular enough.
**Summary:** Task: US-104 | Files: [scripts/score-templates.sh, tests/test-score-templates.sh] | Tests: PASS (30/30) | Review: PASSED | Next: US-105
---

## Iteration 5 - US-105: Backfill historical runs
- Created `scripts/backfill.sh` (~55 lines): iterates all run records through feedback-collector.sh, then generates template-scores.json via score-templates.sh
- Delegates entirely to existing scripts (feedback-collector.sh, score-templates.sh) — zero logic duplication
- Tracks processed/skipped/errors counts with summary output
- Env vars (FEEDBACK_DIR, SCORES_DIR, REGISTRY_FILE) exported for sub-scripts
- Fixed `score-templates.sh`: added `select(.bead != null)` filter to exclude non-feedback JSON files (pattern-registry.json coexists in feedback dir)
- Created `tests/test-backfill.sh` with 19 assertions: usage, missing dir, empty dir, single run, multi-template, running skip, registry seeding, summary output, idempotency
- Smoke-tested against real 102 run records: 90 processed, 12 skipped (running), 0 errors, 6 templates scored
- All 139 tests pass across 5 suites (27 + 38 + 25 + 30 + 19)
- Learnings: pattern-registry.json in feedback dir gets picked up by *.json glob in score-templates.sh — always filter by schema fields when mixing file types in a directory.
**Summary:** Task: US-105 | Files: [scripts/backfill.sh, tests/test-backfill.sh, scripts/score-templates.sh] | Tests: PASS (19/19) | Review: PASSED | Next: US-REVIEW-S1
---

## Iteration 6 - US-REVIEW-S1: Review Sprint 1
- Reviewed all 4 implementation scripts and 5 test files against Linus criteria (good taste, simplicity, no special cases, no duplication)
- Ran all 139 tests across 5 suites: all pass (27+38+25+30+19)
- Acceptance criteria verified: feedback records generated correctly, template-scores.json accurate, all scripts standalone+executable, schema_version present in schemas and outputs
- Cross-task duplication analysis: field extraction shared between feedback-collector.sh and detect-patterns.sh is acceptable (standalone requirement); test helpers duplicated across test files also acceptable (standalone test convention)
- Data flow consistent: backfill.sh → feedback-collector.sh → detect-patterns.sh → score-templates.sh
- Naming conventions consistent across all scripts (FEEDBACK_DIR, SCORES_DIR, REGISTRY_FILE env vars)
- No issues found — Sprint 1 complete
**Summary:** Task: US-REVIEW-S1 | Files: [all Sprint 1 scripts reviewed] | Tests: PASS (139/139) | Review: PASSED | Next: US-201
---

## Iteration 7 - US-201: Build select-template.sh
- Created `scripts/select-template.sh` (~100 lines): recommends template and agent for a task description
- Task type classification via keyword matching: fix/bug→bug-fix, add/create→feature, refactor→refactor, doc→docs, script→script, review→code-review, fallback→custom
- Score lookup from template-scores.json with confidence gating (low confidence triggers warning)
- Agent recommendation: picks agent with highest full_pass_rate (min 3 runs filter)
- Handles missing scores file and no-match cases gracefully with warnings
- Output: JSON with template, variant, agent, model, task_type, score, confidence, reasoning, warnings
- SCORES_DIR env var for testability, consistent with other scripts
- Created `tests/test-select-template.sh` with 27 assertions covering: usage, all 7 task type classifications, output format, confidence gating, agent selection (highest pass rate + min runs filter), missing scores, no matching template, warnings array
- All 166 tests pass across 6 suites (27+38+25+30+19+27)
- Learnings: jq `(.arr[] | select(...)) as $var` produces no output when select finds nothing — use `[.arr[] | select(...)] | if length > 0 then .[0] else null end` to get null instead.
**Summary:** Task: US-201 | Files: [scripts/select-template.sh, tests/test-select-template.sh] | Tests: PASS (27/27) | Review: PASSED | Next: US-202
---

## Iteration 8 - US-202: Build agent-scores.json aggregation
- Extended `scripts/score-templates.sh` to also produce `state/scores/agent-scores.json`
- Refactored from two separate jq invocations (with duplicated compute_score/clamp) into a single combined pipeline that outputs both `template_scores` and `agent_scores`, then splits via bash
- Per-agent global stats: total_runs, pass_rate (full_pass_rate excl. infra), score, avg_duration_ratio, top_failure_patterns (sorted by count, top 5)
- Per-agent per-template breakdown with template name, total_runs, score, full_pass_rate
- Infra failures excluded from pass_rate (consistent with template scoring)
- Created `tests/test-agent-scores.sh` with 24 assertions covering: empty input, single agent stats, avg_duration, top failure patterns, per-template breakdown, multiple agents, infra exclusion, template-scores still generated
- All 190 tests pass across 7 suites (27+38+25+30+19+27+24)
- Learnings: When two jq invocations share helper functions, merge into a single pipeline producing a combined object, then split with bash — eliminates duplication cleanly.
**Summary:** Task: US-202 | Files: [scripts/score-templates.sh, tests/test-agent-scores.sh] | Tests: PASS (24/24) | Review: PASSED | Next: US-203
---

## Iteration 9 - US-203: Integration hook for dispatch.sh
- Created `scripts/dispatch-integration.patch` (~40 lines of changes): unified diff adding `--auto-select` flag to dispatch.sh
- Patch calls `select-template.sh` with prompt text to get data-driven template/agent recommendation
- Advisory mode: logs recommendation but preserves explicit template/agent args passed by user
- Template only applied when user didn't pass explicit template (TEMPLATE_NAME still "custom")
- Agent recommendation is always advisory-only — logged but never overrides explicit agent type
- LEARNING_LOOP_DIR env var configures path to learning-loop project (defaults to sibling dir)
- Created `tests/test-dispatch-integration.sh` with 19 assertions covering: patch existence, content (flag/script/advisory/explicit refs), recommendation output (bug-fix/feature classification, agent selection), advisory mode logic (explicit args preserved), patch format, missing scores handling, documentation
- All 209 tests pass across 8 suites (27+38+25+30+19+27+24+19)
- Learnings: Integration patches are a clean way to propose changes to external scripts without modifying them directly — keeps learning-loop self-contained.
**Summary:** Task: US-203 | Files: [scripts/dispatch-integration.patch, tests/test-dispatch-integration.sh] | Tests: PASS (19/19) | Review: PASSED | Next: US-204
---

## Iteration 10 - US-204: Validate with 10 test dispatches
- Created `scripts/validate-selection.sh` (~150 lines): runs select-template.sh against 10 real task descriptions, compares classifications to expected types, generates validation report
- 10 diverse runs tested across 5 task types: bug-fix (3), feature (3), docs (1), code-review (2), script (1)
- Classification accuracy: 80% (8/10) — two misclassifications from keyword priority order (fix matches before review/script)
- Edge cases documented: keyword priority misclassification, template name mismatch (most runs are `custom`), score data sparsity, multi-intent prompts
- Report written to `state/reports/selection-validation.md` with results table, per-type accuracy, and edge case analysis
- Created `tests/test-selection-validation.sh` with 26 assertions covering: script existence, report generation/structure, classification accuracy (10 prompts), output format, report completeness (≥10 cases), edge case documentation
- All 235 tests pass across 9 suites (27+38+25+30+19+27+24+19+26)
- Learnings: Keyword-priority classification (case/esac first-match) has inherent ordering bias — "fix" appearing anywhere in a review prompt causes misclassification. For advisory mode, 80% accuracy is acceptable.
**Summary:** Task: US-204 | Files: [scripts/validate-selection.sh, tests/test-selection-validation.sh, state/reports/selection-validation.md] | Tests: PASS (26/26) | Review: PASSED | Next: US-REVIEW-S2
---

## Iteration 11 - US-REVIEW-S2: Review Sprint 2
- Reviewed all 4 Sprint 2 tasks (US-201 through US-204): select-template.sh, score-templates.sh (agent-scores extension), dispatch-integration.patch, validate-selection.sh
- All 96 Sprint 2 tests pass (27+24+19+26) across 4 test suites
- Acceptance criteria verified:
  - Task classification accuracy: 80% on 10 real prompts (acceptable for advisory mode), all 7 classification paths tested
  - Agent recommendations correctly use per-template breakdown with min 3 runs filter + highest full_pass_rate sort
  - Dispatch integration is advisory-only: --auto-select defaults false, explicit args always preserved
  - Edge cases covered: no scores file, low confidence warnings, no matching template, missing data fallbacks
- Linus criteria: good taste (single jq pipelines, shared helpers), no special cases, no duplication (compute_score/clamp defined once), clean data flow (score-templates → template-scores.json → select-template → dispatch patch)
- Cross-task: consistent SCORES_DIR env var, clean separation of concerns, agent-scores.json separate from template-level agent breakdown
- Minor note: validate-selection.sh runs classification twice (for overall and per-type accuracy) — acceptable for one-off validation script
- No issues found — Sprint 2 complete
**Summary:** Task: US-REVIEW-S2 | Files: [all Sprint 2 scripts reviewed] | Tests: PASS (96/96) | Review: PASSED | Next: US-301
---

## Iteration 12 - US-301: Build refine-prompts.sh
- Created `scripts/refine-prompts.sh` (~165 lines): identifies underperforming templates and generates refined variants
- Three trigger conditions: full_pass_rate < 0.50 (≥10 runs), pattern count ≥ 5, declining trend
- Pattern-to-strategy mapping via case/esac: test-failure, lint-failure, scope-creep, incomplete-work, repeated-failure, verification-gap each get specific instruction text
- Three modes: preview (default, shows what would change), --dry-run (shows triggers/patterns), --auto (generates variant files + logs)
- Variant naming: auto-increments (bug-fix-v1, bug-fix-v2, ...) by checking existing files
- Refinement log: `state/scores/refinement-log.json` accumulates entries with template, variant, trigger, timestamp, score data, and patterns applied
- Env vars (SCORES_DIR, TEMPLATES_DIR, REGISTRY_FILE, REFINEMENT_LOG) for testability, consistent with other scripts
- Created `tests/test-refine-prompts.sh` with 32 assertions covering: usage, no scores, no refinement needed, all 3 trigger types, --auto/--dry-run/preview modes, variant creation/naming/content, refinement log structure/accumulation, all 6 pattern strategies, multi-pattern application, missing template warning, log timestamps
- All 267 tests pass across 10 suites (27+38+25+30+19+27+24+19+26+32)
- Learnings: jq number formatting varies (0.3 vs 0.30) — use numeric comparison rather than string equality when asserting floating point values in tests.
**Summary:** Task: US-301 | Files: [scripts/refine-prompts.sh, tests/test-refine-prompts.sh] | Tests: PASS (32/32) | Review: PASSED | Next: US-302
---

## Iteration 13 - US-302: Implement A/B test lifecycle
- Created `scripts/ab-tests.sh` (~200 lines): subcommand-based A/B test lifecycle manager with create, pick, record, evaluate, list commands
- State tracked in `state/scores/ab-tests.json` with schema_version, tests array (original, variant, status, target_runs, original_runs, variant_runs)
- Alternating dispatch via `pick` subcommand: returns whichever template has fewer runs (original on tie), outputs JSON with ab_test flag
- Evaluation logic: compares template-scores.json scores for original vs variant, promotes if diff >= 0.1, otherwise discards
- Promotion: archives original to `templates/.archive/`, copies variant to original name, archives variant
- Discard: archives variant to `templates/.archive/`
- All decisions logged to `state/scores/refinement-log.json` with type "ab_test_result", scores, and timestamps
- Integrated with `select-template.sh`: added A/B test check via `ab-tests.sh pick` call, variant field populated when ab_test active
- Created `tests/test-ab-tests.sh` with 45 assertions covering: usage, create (basic, custom target_runs, append), pick (alternation, no test), record (increment counts), evaluate (promote/discard/skip incomplete/skip completed), archive (promote archives original, discard archives variant), refinement-log entries, list (active/empty)
- All 290 tests pass across 11 suites (45+24+19+25+19+38+32+27+30+26+27)
- Learnings: jq `--arg field` + `.[$field] = ...` in update expressions causes syntax errors — use explicit field names in separate branches instead.
**Summary:** Task: US-302 | Files: [scripts/ab-tests.sh, tests/test-ab-tests.sh, scripts/select-template.sh] | Tests: PASS (45/45) | Review: PASSED | Next: US-303
---

## Iteration 14 - US-303: Wire cron jobs
- Created `config/crontab.txt`: declarative template with `__PROJECT_DIR__` placeholders for 3 cron entries (hourly score-templates, daily refine-prompts --auto at 03:00, weekly strategy at Sunday 00:00)
- Created `scripts/install-cron.sh` (~50 lines): installs/removes cron entries using marker-based idempotent merge, supports --dry-run, --remove, --help
- All log output redirected to `state/logs/` per-script log files
- Created `tests/test-install-cron.sh` with 21 assertions covering: crontab.txt existence/format/schedules/paths/log-redirection, install-cron.sh existence/executable/usage/dry-run/remove/marker/paths
- All 311 tests pass across 12 suites (27+38+25+30+19+27+24+19+26+32+45+21)
- Learnings: Marker-based crontab management (append marker comment to each line, grep -v to remove) is clean and idempotent — no need for temp files or lock mechanisms.
**Summary:** Task: US-303 | Files: [config/crontab.txt, scripts/install-cron.sh, tests/test-install-cron.sh] | Tests: PASS (21/21) | Review: PASSED | Next: US-304
---

## Iteration 15 - US-304: Build weekly-strategy.sh
- Created `scripts/weekly-strategy.sh` (~80 lines): generates weekly strategy report from all learning loop state data
- Single jq pipeline produces complete report JSON with 7 sections: template_trends, agent_comparison, top_failure_patterns, ab_results, refinement_activity, recommendations, summary
- Reads 5 input sources: template-scores.json, agent-scores.json, pattern-registry.json, refinement-log.json, ab-tests.json (all optional with defaults)
- Report file: `state/reports/strategy-YYYY-WNN.json` with ISO week numbering
- Recommendations auto-generated: underperforming templates, top failure pattern, agent performance gaps
- Human-readable summary printed to stdout
- SCORES_DIR, FEEDBACK_DIR, REPORTS_DIR env vars for testability, consistent with all other scripts
- Created `tests/test-weekly-strategy.sh` with 37 assertions covering: existence/executable, usage, missing scores dir, report generation, all 7 JSON sections (structure + content), A/B results with active/completed tests, refinement activity, file naming format, stdout summary
- All 348 tests pass across 13 suites (27+38+25+30+19+27+24+19+26+32+45+21+37)
- Learnings: jq `to_entries` on a flat object (pattern-registry.json) is cleaner than trying to iterate keys — converts {key: {count: N}} to array of {key, value} pairs naturally.
**Summary:** Task: US-304 | Files: [scripts/weekly-strategy.sh, tests/test-weekly-strategy.sh] | Tests: PASS (37/37) | Review: PASSED | Next: US-REVIEW-S3
---

## Iteration 16 - US-REVIEW-S3: Review Sprint 3
- Reviewed all 4 Sprint 3 tasks (US-301 through US-304): refine-prompts.sh, ab-tests.sh, install-cron.sh + crontab.txt, weekly-strategy.sh
- All 135 Sprint 3 tests pass (32+45+21+37) across 4 test suites
- Acceptance criteria verified:
  - Refinement triggers fire correctly: low_pass_rate (<0.50, ≥10 runs), pattern count ≥5, declining trend — all with correct threshold gating
  - A/B test tracking: create/pick/record/evaluate lifecycle, alternating dispatch, promote at ≥0.1 diff, archive on promote/discard, refinement-log entries
  - Cron schedules well-separated (hourly/daily 03:00/weekly Sunday 00:00), marker-based idempotent install, log redirection
  - Weekly report has 7 JSON sections (template_trends, agent_comparison, top_failure_patterns, ab_results, refinement_activity, recommendations, summary), ISO week naming, human-readable stdout
- Linus criteria: good taste (single jq pipelines, clean subcommand dispatch), no special cases (fallback defaults), simplicity (80-230 lines each), no duplication (shared refinement-log format, factored log_decision)
- Cross-task: consistent env vars (SCORES_DIR, TEMPLATES_DIR, REFINEMENT_LOG), clean data flow (refine→variant→ab-test→evaluate→log), select-template.sh A/B integration correct
- No issues found — Sprint 3 complete
**Summary:** Task: US-REVIEW-S3 | Files: [all Sprint 3 scripts reviewed] | Tests: PASS (135/135) | Review: PASSED | Next: US-401
---

## Iteration 17 - US-401: Notification integration
- Created `scripts/notify.sh` (~95 lines): wrapper around wake-gateway.sh with event-type message formatting
- 5 event types: variant-created, variant-promoted, variant-discarded, score-regression, weekly-report
- All messages prefixed with "Learning Loop:" for consistent identification
- Supports --dry-run (show message without sending) and NOTIFY_ENABLED=false (skip all notifications)
- WAKE_GATEWAY env var overrides gateway path for testability
- Gateway failures caught gracefully (2>/dev/null || true in callers) — notifications never crash the main workflow
- Integrated into 4 existing scripts:
  - refine-prompts.sh: notifies on variant creation (--auto mode)
  - ab-tests.sh: notifies on variant promoted and variant discarded
  - weekly-strategy.sh: notifies with weekly summary text
  - score-templates.sh: detects score regression (>0.1 drop, ≥10 runs) before overwriting scores file
- Created `tests/test-notify.sh` with 28 assertions covering: existence, usage, all 5 event types (message content), dry-run mode, gateway failure handling, NOTIFY_ENABLED=false, unknown event error
- All 376 tests pass across 14 suites (27+38+25+30+19+27+24+19+26+32+45+21+28+37)
- Learnings: Notification wrappers should never crash callers — always use `2>/dev/null || true` on integration calls. Score regression detection belongs in the scoring script (where old scores are available before overwrite), not in a separate scheduled job.
**Summary:** Task: US-401 | Files: [scripts/notify.sh, tests/test-notify.sh, scripts/refine-prompts.sh, scripts/ab-tests.sh, scripts/weekly-strategy.sh, scripts/score-templates.sh] | Tests: PASS (28/28) | Review: PASSED | Next: US-402
---

## Iteration 18 - US-402: Safety guardrails
- Created `scripts/guardrails.sh` (~230 lines): subcommand-based safety guardrails with 9 subcommands
- 6 guardrail categories implemented:
  1. Max 3 active variants per template: `check-variant-limit` / `enforce-variant-limit` discards oldest active variants beyond limit
  2. Minimum sample size enforcement: `check-sample-size` validates ≥5 for scoring, ≥10 for refinement
  3. Auto-rollback: `check-rollback` / `auto-rollback` detects promoted templates scoring worse than pre-promotion and restores archived original
  4. `NO_AUTO_PROMOTE` env var: `check-promote` gates promotions for human review; integrated into ab-tests.sh evaluate
  5. Prompt hash tracking: `check-duplicates` / `count-unique` find duplicate prompt_hash entries for dedup awareness
  6. Refinement loop breaker: `check-refinement-loop` flags templates with 5+ refinements without improvement for human review
- Integrated into existing scripts:
  - refine-prompts.sh: calls `check-refinement-loop` before processing each candidate template
  - ab-tests.sh: calls `enforce-variant-limit` on create, checks `NO_AUTO_PROMOTE` before promoting
- Constants at top of guardrails.sh: MAX_ACTIVE_VARIANTS=3, MIN_SCORING_RUNS=5, MIN_REFINEMENT_RUNS=10, ROLLBACK_RUN_THRESHOLD=10, REFINEMENT_LOOP_LIMIT=5
- Created `tests/test-guardrails.sh` with 26 assertions covering: existence, usage, variant limit (check/enforce/archive), sample size (scoring/refinement thresholds), auto-rollback (detection/restore), no-auto-promote (gating/allow), prompt hash dedup (detection/count-unique), refinement loop (breaker/clear/improving), integration with refine-prompts.sh and ab-tests.sh
- All 402 tests pass across 15 suites (27+38+25+30+19+27+24+19+26+32+45+21+28+37+26)
- Learnings: Guardrails integrate as lightweight callouts (`"$GUARDRAILS" subcommand` with `|| true` fallback) — keeps existing scripts clean. Archive naming convention matters for rollback (use consistent names like `template.md` in `.archive/`).
**Summary:** Task: US-402 | Files: [scripts/guardrails.sh, tests/test-guardrails.sh, scripts/refine-prompts.sh, scripts/ab-tests.sh] | Tests: PASS (26/26) | Review: PASSED | Next: US-403
---
